{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==============================================================================\n# SCRIPT D'ENTRAÎNEMENT FINAL - VERSION DÉFINITIVE ET CORRIGÉE\n# ==============================================================================\n\n# --- 1. SETUP AND IMPORTS ---\nimport os\nimport gc\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, applications, regularizers\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.optimizers import AdamW\nfrom tensorflow.keras.optimizers.schedules import CosineDecay\nimport matplotlib.pyplot as plt\n\nprint(f\"TensorFlow Version: {tf.__version__}\")\n\n# --- 2. DATASET DOWNLOAD FROM GOOGLE DRIVE ---\n!pip install -q gdown\ngdrive_link = \"https://drive.google.com/file/d/1oaJcF-Oe-81OD9wp16VExOJgLuc8vU94/view?usp=drive_link\"\noutput_zip_path = \"/kaggle/working/dataset.zip\"\nextract_path = \"/kaggle/working/datasets/\"\nprint(\"📂 Téléchargement du dataset...\")\n!gdown --fuzzy \"{gdrive_link}\" -O \"{output_zip_path}\"\nprint(\"📦 Décompression du dataset...\")\n!unzip -q -o \"{output_zip_path}\" -d \"{extract_path}\"\nprint(f\"✅ Dataset prêt dans {extract_path}\")\n\n# --- 3. CONFIGURATION ---\nIMG_SIZE = 224\nBATCH_SIZE = 24\nEPOCHS_PHASE_1 = 20\nEPOCHS_PHASE_2 = 40\nAUTOTUNE = tf.data.AUTOTUNE\n\nTRAIN_DIR = os.path.join(extract_path, \"train\")\nTEST_DIR = os.path.join(extract_path, \"test\")\nCHECKPOINT_DIR = \"/kaggle/working/AI_Checkpoints\"\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\nCHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, \"emotion_efficientnetv2M_SOTA.weights.h5\")\n\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\nprint(\"\\n✅ Mixed Precision Training Enabled.\")\nprint(f\"📐 Image Resolution: {IMG_SIZE}x{IMG_SIZE}\")\n\n# --- 4. DATA PREPARATION & AUGMENTATION ---\ndata_augmentation = tf.keras.Sequential([\n    layers.RandomFlip(\"horizontal\"), layers.RandomRotation(0.15),\n    layers.RandomZoom(0.15), layers.RandomTranslation(0.1, 0.1),\n    layers.RandomContrast(0.15),\n], name=\"data_augmentation\")\n\ndef cutout(images, labels):\n    image_height = tf.shape(images)[1]\n    image_width = tf.shape(images)[2]\n    batch_size = tf.shape(images)[0]\n    scale=(0.02, 0.1); ratio=(0.3, 3.3)\n    area = tf.cast(image_height * image_width, tf.float32)\n    erase_area = tf.random.uniform([batch_size], scale[0], scale[1]) * area\n    aspect_ratio = tf.random.uniform([batch_size], ratio[0], ratio[1])\n    h = tf.cast(tf.round(tf.sqrt(erase_area * aspect_ratio)), tf.int32)\n    w = tf.cast(tf.round(tf.sqrt(erase_area / aspect_ratio)), tf.int32)\n    h = tf.minimum(h, image_height); w = tf.minimum(w, image_width)\n    x_rate = tf.random.uniform([batch_size]); y_rate = tf.random.uniform([batch_size])\n    x = tf.cast(x_rate * tf.cast(image_width - w, tf.float32), tf.int32)\n    y = tf.cast(y_rate * tf.cast(image_height - h, tf.float32), tf.int32)\n\n    # --- CORRECTION DE BROADCASTING ---\n    # Créer les grilles de coordonnées pour toutes les images\n    row_coords = tf.cast(tf.range(image_height), dtype=tf.int32)\n    col_coords = tf.cast(tf.range(image_width), dtype=tf.int32)\n    \n    # Adapter les formes pour le broadcasting (la forme correcte est (1, H, 1, 1) et (1, 1, W, 1))\n    row_coords = tf.reshape(row_coords, (1, image_height, 1, 1))\n    col_coords = tf.reshape(col_coords, (1, 1, image_width, 1))\n    # --- FIN DE LA CORRECTION ---\n\n    y_b = tf.reshape(y, (batch_size, 1, 1, 1)); h_b = tf.reshape(h, (batch_size, 1, 1, 1))\n    x_b = tf.reshape(x, (batch_size, 1, 1, 1)); w_b = tf.reshape(w, (batch_size, 1, 1, 1))\n    \n    mask_y = tf.logical_and(row_coords >= y_b, row_coords < y_b + h_b)\n    mask_x = tf.logical_and(col_coords >= x_b, col_coords < x_b + w_b)\n    mask = tf.cast(tf.logical_and(mask_y, mask_x), images.dtype)\n    return images * (1.0 - mask), labels\n\ndef create_dataset(directory, augment=False):\n    initial_ds = tf.keras.utils.image_dataset_from_directory(\n        directory, label_mode='categorical', image_size=(IMG_SIZE, IMG_SIZE),\n        batch_size=BATCH_SIZE, shuffle=True if augment else False\n    )\n    class_names = initial_ds.class_names\n    dataset = initial_ds.map(lambda x, y: (tf.cast(x, tf.float32), y), num_parallel_calls=AUTOTUNE)\n    if augment:\n        dataset = dataset.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=AUTOTUNE)\n        dataset = dataset.map(cutout, num_parallel_calls=AUTOTUNE)\n    return dataset.prefetch(buffer_size=AUTOTUNE), class_names\n\nprint(\"\\nLoading and preparing datasets...\")\ntrain_dataset, class_names = create_dataset(TRAIN_DIR, augment=True)\ntest_dataset, _ = create_dataset(TEST_DIR, augment=False)\nNUM_CLASSES = len(class_names)\nprint(f\"✅ Found {NUM_CLASSES} classes: {class_names}\")\n\n# --- 5. MODEL DEFINITION ---\ninputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\nbase_model = applications.EfficientNetV2M(\n    input_shape=(IMG_SIZE, IMG_SIZE, 3), include_top=False, weights=\"imagenet\"\n)\nbase_model.trainable = False\nx = base_model(inputs, training=False)\nx = layers.GlobalAveragePooling2D()(x)\nx = layers.Dense(768, activation=\"gelu\", kernel_regularizer=regularizers.l2(1e-4))(x)\nx = layers.BatchNormalization()(x)\nx = layers.Dropout(0.5)(x)\noutputs = layers.Dense(NUM_CLASSES, activation=\"softmax\", dtype=tf.float32)(x)\nmodel = models.Model(inputs, outputs)\n\nprint(\"\\nBuilding SOTA model with EfficientNetV2-M base...\")\nmodel.summary()\n\n# --- 6. ADVANCED TRAINING STRATEGY ---\n@tf.keras.utils.register_keras_serializable()\nclass FocalLoss(tf.keras.losses.Loss):\n    def __init__(self, gamma=2.0, alpha=0.25, name='focal_loss'):\n        super().__init__(name=name)\n        self.gamma = gamma; self.alpha = alpha\n    def call(self, y_true, y_pred):\n        ce = tf.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=False)\n        p_t = tf.reduce_sum(y_true * y_pred, axis=-1)\n        loss = self.alpha * tf.pow(1.0 - p_t, self.gamma) * ce\n        return tf.reduce_mean(loss)\n\n# --- PHASE 1 ---\nprint(\"\\n\" + \"=\"*50); print(\"🚀 PHASE 1: Training the SOTA Head\"); print(\"=\"*50)\ntotal_steps_phase1 = len(train_dataset) * EPOCHS_PHASE_1\nlr_schedule_phase1 = CosineDecay(initial_learning_rate=1e-3, decay_steps=total_steps_phase1, alpha=0.01)\ncallbacks_phase1 = [\n    EarlyStopping(monitor='val_accuracy', patience=8, restore_best_weights=True, verbose=1),\n    ModelCheckpoint(filepath=CHECKPOINT_PATH, monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n]\nmodel.compile(optimizer=AdamW(learning_rate=lr_schedule_phase1, weight_decay=1e-4), loss=FocalLoss(), metrics=[\"accuracy\"])\nhistory_phase1 = model.fit(train_dataset, validation_data=test_dataset, epochs=EPOCHS_PHASE_1, callbacks=callbacks_phase1)\n\n# --- PHASE 2 ---\nprint(\"\\n\" + \"=\"*50); print(\"🔧 PHASE 2: Full Network Fine-Tuning\"); print(\"=\"*50)\nbase_model.trainable = True\nfine_tune_at = int(len(base_model.layers) * 0.50)\nfor layer in base_model.layers[:fine_tune_at]:\n    layer.trainable = False\nfor layer in base_model.layers:\n    if isinstance(layer, layers.BatchNormalization): layer.trainable = False\nprint(f\"🔓 Unfrozen {len(base_model.layers) - fine_tune_at} layers.\")\ntotal_steps_phase2 = len(train_dataset) * EPOCHS_PHASE_2\nlr_schedule_phase2 = CosineDecay(initial_learning_rate=1e-5, decay_steps=total_steps_phase2, alpha=0.1)\ncallbacks_phase2 = [\n    EarlyStopping(monitor='val_accuracy', patience=12, restore_best_weights=True, verbose=1),\n    ModelCheckpoint(filepath=CHECKPOINT_PATH, monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n]\nmodel.compile(optimizer=AdamW(learning_rate=lr_schedule_phase2, weight_decay=1e-5), loss=FocalLoss(), metrics=[\"accuracy\"])\nhistory_phase2 = model.fit(train_dataset, validation_data=test_dataset,\n    epochs=EPOCHS_PHASE_2,\n    initial_epoch=history_phase1.epoch[-1] if history_phase1.epoch and len(history_phase1.epoch) > 0 else 0,\n    callbacks=callbacks_phase2)\n\n# --- 7. FINAL EVALUATION AND SAVING (CORRECTED & SIMPLIFIED) ---\nprint(\"\\n\" + \"=\"*50); print(\"📊 FINAL EVALUATION\"); print(\"=\"*50)\nprint(\"✅ Best model weights are already in memory from training.\")\ntest_loss, test_acc = model.evaluate(test_dataset, verbose=1)\nprint(f\"\\n🎯 Final Test Accuracy: {test_acc*100:.2f}%\")\n\nfinal_model_name = f\"sota_emotion_model_final_acc_{test_acc*100:.2f}.keras\"\nfinal_model_path = os.path.join(CHECKPOINT_DIR, final_model_name)\nmodel.save(final_model_path)\nprint(f\"💾 Model saved to: {final_model_path}\")\nprint(\"\\n🎉 ENTRAÎNEMENT TERMINÉ ET MODÈLE SAUVEGARDÉ AVEC SUCCÈS !\")\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T18:49:00.698058Z","iopub.execute_input":"2025-08-23T18:49:00.698746Z","iopub.status.idle":"2025-08-23T23:36:20.086476Z","shell.execute_reply.started":"2025-08-23T18:49:00.698719Z","shell.execute_reply":"2025-08-23T23:36:20.085831Z"}},"outputs":[{"name":"stderr","text":"2025-08-23 18:49:03.714260: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755974944.086553      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755974944.199825      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"TensorFlow Version: 2.18.0\n📂 Téléchargement du dataset...\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1oaJcF-Oe-81OD9wp16VExOJgLuc8vU94\nFrom (redirected): https://drive.google.com/uc?id=1oaJcF-Oe-81OD9wp16VExOJgLuc8vU94&confirm=t&uuid=49e5c91e-439a-4111-9e06-dd5de525de48\nTo: /kaggle/working/dataset.zip\n100%|████████████████████████████████████████| 117M/117M [00:01<00:00, 92.3MB/s]\n📦 Décompression du dataset...\n✅ Dataset prêt dans /kaggle/working/datasets/\n\n✅ Mixed Precision Training Enabled.\n📐 Image Resolution: 224x224\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1755974977.055593      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1755974977.056514      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"\nLoading and preparing datasets...\nFound 41882 files belonging to 7 classes.\nFound 10246 files belonging to 7 classes.\n✅ Found 7 classes: ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-m_notop.h5\n\u001b[1m214201816/214201816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n\nBuilding SOTA model with EfficientNetV2-M base...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ efficientnetv2-m (\u001b[38;5;33mFunctional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │    \u001b[38;5;34m53,150,388\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │       \u001b[38;5;34m983,808\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │         \u001b[38;5;34m3,072\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ cast_1 (\u001b[38;5;33mCast\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │         \u001b[38;5;34m5,383\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ efficientnetv2-m (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">53,150,388</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">983,808</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,072</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ cast_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,383</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m54,142,651\u001b[0m (206.54 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">54,142,651</span> (206.54 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m990,727\u001b[0m (3.78 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">990,727</span> (3.78 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m53,151,924\u001b[0m (202.76 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">53,151,924</span> (202.76 MB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"\n==================================================\n🚀 PHASE 1: Training the SOTA Head\n==================================================\nEpoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1755975048.976609     110 service.cc:148] XLA service 0x7b26d40f1b20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1755975048.978010     110 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1755975048.978034     110 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1755975056.104771     110 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m   1/1746\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m50:29:30\u001b[0m 104s/step - accuracy: 0.0417 - loss: 1.0191","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1755975093.318191     110 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step - accuracy: 0.3448 - loss: 0.5313\nEpoch 1: val_accuracy improved from -inf to 0.45159, saving model to /kaggle/working/AI_Checkpoints/emotion_efficientnetv2M_SOTA.weights.h5\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m580s\u001b[0m 273ms/step - accuracy: 0.3448 - loss: 0.5313 - val_accuracy: 0.4516 - val_loss: 0.3415\nEpoch 2/20\n\u001b[1m1745/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.4011 - loss: 0.3473\nEpoch 2: val_accuracy improved from 0.45159 to 0.48048, saving model to /kaggle/working/AI_Checkpoints/emotion_efficientnetv2M_SOTA.weights.h5\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 233ms/step - accuracy: 0.4011 - loss: 0.3473 - val_accuracy: 0.4805 - val_loss: 0.2981\nEpoch 3/20\n\u001b[1m1745/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.4093 - loss: 0.3214\nEpoch 3: val_accuracy did not improve from 0.48048\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m440s\u001b[0m 232ms/step - accuracy: 0.4093 - loss: 0.3214 - val_accuracy: 0.4786 - val_loss: 0.2900\nEpoch 4/20\n\u001b[1m1745/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.4174 - loss: 0.3135\nEpoch 4: val_accuracy did not improve from 0.48048\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m406s\u001b[0m 232ms/step - accuracy: 0.4174 - loss: 0.3135 - val_accuracy: 0.4790 - val_loss: 0.2861\nEpoch 5/20\n\u001b[1m1745/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.4195 - loss: 0.3040\nEpoch 5: val_accuracy did not improve from 0.48048\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 234ms/step - accuracy: 0.4195 - loss: 0.3040 - val_accuracy: 0.4780 - val_loss: 0.2793\nEpoch 6/20\n\u001b[1m1745/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.4243 - loss: 0.2951\nEpoch 6: val_accuracy improved from 0.48048 to 0.49180, saving model to /kaggle/working/AI_Checkpoints/emotion_efficientnetv2M_SOTA.weights.h5\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m408s\u001b[0m 233ms/step - accuracy: 0.4243 - loss: 0.2951 - val_accuracy: 0.4918 - val_loss: 0.2638\nEpoch 7/20\n\u001b[1m1745/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.4287 - loss: 0.2880\nEpoch 7: val_accuracy improved from 0.49180 to 0.49502, saving model to /kaggle/working/AI_Checkpoints/emotion_efficientnetv2M_SOTA.weights.h5\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m411s\u001b[0m 235ms/step - accuracy: 0.4287 - loss: 0.2879 - val_accuracy: 0.4950 - val_loss: 0.2535\nEpoch 8/20\n\u001b[1m1745/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.4347 - loss: 0.2787\nEpoch 8: val_accuracy improved from 0.49502 to 0.50488, saving model to /kaggle/working/AI_Checkpoints/emotion_efficientnetv2M_SOTA.weights.h5\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m441s\u001b[0m 234ms/step - accuracy: 0.4347 - loss: 0.2787 - val_accuracy: 0.5049 - val_loss: 0.2441\nEpoch 9/20\n\u001b[1m1745/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.4371 - loss: 0.2741\nEpoch 9: val_accuracy improved from 0.50488 to 0.50732, saving model to /kaggle/working/AI_Checkpoints/emotion_efficientnetv2M_SOTA.weights.h5\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m412s\u001b[0m 236ms/step - accuracy: 0.4371 - loss: 0.2741 - val_accuracy: 0.5073 - val_loss: 0.2417\nEpoch 10/20\n\u001b[1m1745/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.4455 - loss: 0.2674\nEpoch 10: val_accuracy improved from 0.50732 to 0.51913, saving model to /kaggle/working/AI_Checkpoints/emotion_efficientnetv2M_SOTA.weights.h5\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 233ms/step - accuracy: 0.4455 - loss: 0.2674 - val_accuracy: 0.5191 - val_loss: 0.2328\nEpoch 11/20\n\u001b[1m1745/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.4550 - loss: 0.2613\nEpoch 11: val_accuracy did not improve from 0.51913\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m410s\u001b[0m 234ms/step - accuracy: 0.4550 - loss: 0.2613 - val_accuracy: 0.5098 - val_loss: 0.2320\nEpoch 12/20\n\u001b[1m1745/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.4521 - loss: 0.2561\nEpoch 12: val_accuracy did not improve from 0.51913\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m442s\u001b[0m 234ms/step - accuracy: 0.4521 - loss: 0.2561 - val_accuracy: 0.5188 - val_loss: 0.2254\nEpoch 13/20\n\u001b[1m1745/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.4561 - loss: 0.2532\nEpoch 13: val_accuracy improved from 0.51913 to 0.52635, saving model to /kaggle/working/AI_Checkpoints/emotion_efficientnetv2M_SOTA.weights.h5\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m414s\u001b[0m 237ms/step - accuracy: 0.4561 - loss: 0.2532 - val_accuracy: 0.5264 - val_loss: 0.2191\nEpoch 14/20\n\u001b[1m1745/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.4602 - loss: 0.2492\nEpoch 14: val_accuracy improved from 0.52635 to 0.53094, saving model to /kaggle/working/AI_Checkpoints/emotion_efficientnetv2M_SOTA.weights.h5\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 234ms/step - accuracy: 0.4602 - loss: 0.2492 - val_accuracy: 0.5309 - val_loss: 0.2184\nEpoch 15/20\n\u001b[1m1745/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.4662 - loss: 0.2461\nEpoch 15: val_accuracy did not improve from 0.53094\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m405s\u001b[0m 232ms/step - accuracy: 0.4662 - loss: 0.2461 - val_accuracy: 0.5296 - val_loss: 0.2124\nEpoch 16/20\n\u001b[1m1745/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.4739 - loss: 0.2430\nEpoch 16: val_accuracy did not improve from 0.53094\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 234ms/step - accuracy: 0.4739 - loss: 0.2430 - val_accuracy: 0.5299 - val_loss: 0.2115\nEpoch 17/20\n\u001b[1m1745/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.4771 - loss: 0.2381\nEpoch 17: val_accuracy improved from 0.53094 to 0.53826, saving model to /kaggle/working/AI_Checkpoints/emotion_efficientnetv2M_SOTA.weights.h5\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m413s\u001b[0m 236ms/step - accuracy: 0.4771 - loss: 0.2381 - val_accuracy: 0.5383 - val_loss: 0.2073\nEpoch 18/20\n\u001b[1m1745/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.4764 - loss: 0.2371\nEpoch 18: val_accuracy did not improve from 0.53826\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m412s\u001b[0m 236ms/step - accuracy: 0.4764 - loss: 0.2371 - val_accuracy: 0.5379 - val_loss: 0.2070\nEpoch 19/20\n\u001b[1m1745/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - accuracy: 0.4778 - loss: 0.2349\nEpoch 19: val_accuracy improved from 0.53826 to 0.53963, saving model to /kaggle/working/AI_Checkpoints/emotion_efficientnetv2M_SOTA.weights.h5\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m428s\u001b[0m 228ms/step - accuracy: 0.4778 - loss: 0.2349 - val_accuracy: 0.5396 - val_loss: 0.2052\nEpoch 20/20\n\u001b[1m1745/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - accuracy: 0.4821 - loss: 0.2339\nEpoch 20: val_accuracy improved from 0.53963 to 0.54060, saving model to /kaggle/working/AI_Checkpoints/emotion_efficientnetv2M_SOTA.weights.h5\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m441s\u001b[0m 227ms/step - accuracy: 0.4822 - loss: 0.2339 - val_accuracy: 0.5406 - val_loss: 0.2050\nRestoring model weights from the end of the best epoch: 20.\n\n==================================================\n🔧 PHASE 2: Full Network Fine-Tuning\n==================================================\n🔓 Unfrozen 370 layers.\nEpoch 20/40\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step - accuracy: 0.5264 - loss: 0.2120\nEpoch 20: val_accuracy improved from -inf to 0.63176, saving model to /kaggle/working/AI_Checkpoints/emotion_efficientnetv2M_SOTA.weights.h5\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m627s\u001b[0m 274ms/step - accuracy: 0.5264 - loss: 0.2120 - val_accuracy: 0.6318 - val_loss: 0.1598\nEpoch 21/40\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.5882 - loss: 0.1796\nEpoch 21: val_accuracy improved from 0.63176 to 0.65304, saving model to /kaggle/working/AI_Checkpoints/emotion_efficientnetv2M_SOTA.weights.h5\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m401s\u001b[0m 229ms/step - accuracy: 0.5882 - loss: 0.1796 - val_accuracy: 0.6530 - val_loss: 0.1473\nEpoch 22/40\n\u001b[1m1745/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.6171 - loss: 0.1660\nEpoch 22: val_accuracy improved from 0.65304 to 0.66816, saving model to /kaggle/working/AI_Checkpoints/emotion_efficientnetv2M_SOTA.weights.h5\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m402s\u001b[0m 230ms/step - accuracy: 0.6171 - loss: 0.1660 - val_accuracy: 0.6682 - val_loss: 0.1421\nEpoch 23/40\n\u001b[1m1745/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.6394 - loss: 0.1557\nEpoch 23: val_accuracy improved from 0.66816 to 0.67929, saving model to /kaggle/working/AI_Checkpoints/emotion_efficientnetv2M_SOTA.weights.h5\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m401s\u001b[0m 229ms/step - accuracy: 0.6394 - loss: 0.1557 - val_accuracy: 0.6793 - val_loss: 0.1363\nEpoch 24/40\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.6570 - loss: 0.1475\nEpoch 24: val_accuracy improved from 0.67929 to 0.69295, saving model to /kaggle/working/AI_Checkpoints/emotion_efficientnetv2M_SOTA.weights.h5\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m408s\u001b[0m 233ms/step - accuracy: 0.6570 - loss: 0.1475 - val_accuracy: 0.6930 - val_loss: 0.1312\nEpoch 25/40\n\u001b[1m1745/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.6667 - loss: 0.1419\nEpoch 25: val_accuracy improved from 0.69295 to 0.69774, saving model to /kaggle/working/AI_Checkpoints/emotion_efficientnetv2M_SOTA.weights.h5\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m426s\u001b[0m 243ms/step - accuracy: 0.6667 - loss: 0.1419 - val_accuracy: 0.6977 - val_loss: 0.1290\nEpoch 26/40\n\u001b[1m1745/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.6788 - loss: 0.1358\nEpoch 26: val_accuracy improved from 0.69774 to 0.70301, saving model to /kaggle/working/AI_Checkpoints/emotion_efficientnetv2M_SOTA.weights.h5\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m414s\u001b[0m 237ms/step - accuracy: 0.6788 - loss: 0.1358 - val_accuracy: 0.7030 - val_loss: 0.1272\nEpoch 27/40\n\u001b[1m1745/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.6898 - loss: 0.1292\nEpoch 27: val_accuracy did not improve from 0.70301\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m402s\u001b[0m 230ms/step - accuracy: 0.6898 - loss: 0.1292 - val_accuracy: 0.6935 - val_loss: 0.1301\nEpoch 28/40\n\u001b[1m1745/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - accuracy: 0.6961 - loss: 0.1265\nEpoch 28: val_accuracy improved from 0.70301 to 0.70525, saving model to /kaggle/working/AI_Checkpoints/emotion_efficientnetv2M_SOTA.weights.h5\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m390s\u001b[0m 223ms/step - accuracy: 0.6961 - loss: 0.1265 - val_accuracy: 0.7053 - val_loss: 0.1265\nEpoch 29/40\n\u001b[1m1745/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.7040 - loss: 0.1219\nEpoch 29: val_accuracy improved from 0.70525 to 0.71501, saving model to /kaggle/working/AI_Checkpoints/emotion_efficientnetv2M_SOTA.weights.h5\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 224ms/step - accuracy: 0.7040 - loss: 0.1219 - val_accuracy: 0.7150 - val_loss: 0.1220\nEpoch 30/40\n\u001b[1m1745/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - accuracy: 0.7109 - loss: 0.1181\nEpoch 30: val_accuracy did not improve from 0.71501\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m388s\u001b[0m 222ms/step - accuracy: 0.7109 - loss: 0.1181 - val_accuracy: 0.7147 - val_loss: 0.1234\nEpoch 31/40\n\u001b[1m1745/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 0.7214 - loss: 0.1131\nEpoch 31: val_accuracy improved from 0.71501 to 0.71765, saving model to /kaggle/working/AI_Checkpoints/emotion_efficientnetv2M_SOTA.weights.h5\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m388s\u001b[0m 222ms/step - accuracy: 0.7214 - loss: 0.1131 - val_accuracy: 0.7176 - val_loss: 0.1203\nEpoch 32/40\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - accuracy: 0.7279 - loss: 0.1110\nEpoch 32: val_accuracy improved from 0.71765 to 0.71989, saving model to /kaggle/working/AI_Checkpoints/emotion_efficientnetv2M_SOTA.weights.h5\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m391s\u001b[0m 223ms/step - accuracy: 0.7279 - loss: 0.1110 - val_accuracy: 0.7199 - val_loss: 0.1210\nEpoch 33/40\n\u001b[1m1745/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.7335 - loss: 0.1074\nEpoch 33: val_accuracy did not improve from 0.71989\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m389s\u001b[0m 222ms/step - accuracy: 0.7335 - loss: 0.1074 - val_accuracy: 0.7186 - val_loss: 0.1226\nEpoch 34/40\n\u001b[1m1745/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.7380 - loss: 0.1045\nEpoch 34: val_accuracy improved from 0.71989 to 0.72038, saving model to /kaggle/working/AI_Checkpoints/emotion_efficientnetv2M_SOTA.weights.h5\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 224ms/step - accuracy: 0.7380 - loss: 0.1045 - val_accuracy: 0.7204 - val_loss: 0.1215\nEpoch 35/40\n\u001b[1m1745/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.7492 - loss: 0.1000\nEpoch 35: val_accuracy improved from 0.72038 to 0.72428, saving model to /kaggle/working/AI_Checkpoints/emotion_efficientnetv2M_SOTA.weights.h5\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 224ms/step - accuracy: 0.7492 - loss: 0.1000 - val_accuracy: 0.7243 - val_loss: 0.1216\nEpoch 36/40\n\u001b[1m1745/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.7490 - loss: 0.1002\nEpoch 36: val_accuracy improved from 0.72428 to 0.72741, saving model to /kaggle/working/AI_Checkpoints/emotion_efficientnetv2M_SOTA.weights.h5\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m391s\u001b[0m 224ms/step - accuracy: 0.7490 - loss: 0.1002 - val_accuracy: 0.7274 - val_loss: 0.1202\nEpoch 37/40\n\u001b[1m1745/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.7582 - loss: 0.0959\nEpoch 37: val_accuracy did not improve from 0.72741\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m389s\u001b[0m 222ms/step - accuracy: 0.7582 - loss: 0.0959 - val_accuracy: 0.7274 - val_loss: 0.1210\nEpoch 38/40\n\u001b[1m1745/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.7655 - loss: 0.0927\nEpoch 38: val_accuracy improved from 0.72741 to 0.73043, saving model to /kaggle/working/AI_Checkpoints/emotion_efficientnetv2M_SOTA.weights.h5\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m422s\u001b[0m 241ms/step - accuracy: 0.7655 - loss: 0.0927 - val_accuracy: 0.7304 - val_loss: 0.1204\nEpoch 39/40\n\u001b[1m1745/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.7701 - loss: 0.0906\nEpoch 39: val_accuracy did not improve from 0.73043\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m408s\u001b[0m 233ms/step - accuracy: 0.7701 - loss: 0.0906 - val_accuracy: 0.7268 - val_loss: 0.1208\nEpoch 40/40\n\u001b[1m1745/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.7764 - loss: 0.0872\nEpoch 40: val_accuracy did not improve from 0.73043\n\u001b[1m1746/1746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m414s\u001b[0m 236ms/step - accuracy: 0.7764 - loss: 0.0872 - val_accuracy: 0.7284 - val_loss: 0.1222\nRestoring model weights from the end of the best epoch: 38.\n\n==================================================\n📊 FINAL EVALUATION\n==================================================\n✅ Best model weights are already in memory from training.\n\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 50ms/step - accuracy: 0.6905 - loss: 0.1408\n\n🎯 Final Test Accuracy: 73.04%\n💾 Model saved to: /kaggle/working/AI_Checkpoints/sota_emotion_model_final_acc_73.04.keras\n\n🎉 ENTRAÎNEMENT TERMINÉ ET MODÈLE SAUVEGARDÉ AVEC SUCCÈS !\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"914"},"metadata":{}}],"execution_count":1}]}