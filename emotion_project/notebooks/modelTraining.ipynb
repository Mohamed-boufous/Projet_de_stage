{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4O3BDLidTiwI",
        "outputId": "9f615d58-9ef6-4057-d20f-57f1559825e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mdb-txVfiV4",
        "outputId": "03bc15f3-2ded-473c-ff02-cd21f2b1f3ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset décompressé avec succès !\n"
          ]
        }
      ],
      "source": [
        "# Chemin vers votre fichier .zip sur Google Drive\n",
        "zip_path = \"/content/drive/MyDrive/AI_Datasets/combined_dataset_96.zip\"\n",
        "\n",
        "# Dossier de destination dans l'environnement Colab\n",
        "extract_path = \"/content/datasets/\"\n",
        "\n",
        "# Commande de décompression\n",
        "!unzip -q \"{zip_path}\" -d \"{extract_path}\"\n",
        "\n",
        "print(\"Dataset décompressé avec succès !\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8I3vBbKRiGjy",
        "outputId": "26446d8f-bbd3-44f8-de1d-a4d9577c01f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 Configuration optimisée pour la mémoire activée.\n",
            "📐 Taille d'image : 128x128\n",
            "📦 Batch size : 16\n",
            "📂 Chargement des datasets...\n",
            "Found 41882 files belonging to 7 classes.\n",
            "Found 10246 files belonging to 7 classes.\n",
            "✅ Classes trouvées : ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
            "🏗️ Création du modèle optimisé...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-b0_notop.h5\n",
            "\u001b[1m24274472/24274472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ efficientnetv2-b0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,919,312</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">327,936</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,799</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ efficientnetv2-b0 (\u001b[38;5;33mFunctional\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m5,919,312\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │         \u001b[38;5;34m5,120\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m327,936\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │         \u001b[38;5;34m1,799\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,254,167</span> (23.86 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,254,167\u001b[0m (23.86 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">332,295</span> (1.27 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m332,295\u001b[0m (1.27 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,921,872</span> (22.59 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m5,921,872\u001b[0m (22.59 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "🚀 PHASE 1 : Entraînement de la tête\n",
            "==================================================\n",
            "Epoch 1/15\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.4162 - loss: 1.7586\n",
            "Epoch 1: val_accuracy improved from -inf to 0.51737, saving model to /content/drive/MyDrive/AI_Checkpoints/emotion_model_optimized.weights.h5\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 24ms/step - accuracy: 0.4162 - loss: 1.7585 - val_accuracy: 0.5174 - val_loss: 1.2749 - learning_rate: 0.0010\n",
            "Epoch 2/15\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4955 - loss: 1.3533\n",
            "Epoch 2: val_accuracy improved from 0.51737 to 0.53094, saving model to /content/drive/MyDrive/AI_Checkpoints/emotion_model_optimized.weights.h5\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 17ms/step - accuracy: 0.4955 - loss: 1.3533 - val_accuracy: 0.5309 - val_loss: 1.2528 - learning_rate: 0.0010\n",
            "Epoch 3/15\n",
            "\u001b[1m2614/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5070 - loss: 1.3275\n",
            "Epoch 3: val_accuracy improved from 0.53094 to 0.53894, saving model to /content/drive/MyDrive/AI_Checkpoints/emotion_model_optimized.weights.h5\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 16ms/step - accuracy: 0.5070 - loss: 1.3275 - val_accuracy: 0.5389 - val_loss: 1.2303 - learning_rate: 0.0010\n",
            "Epoch 4/15\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5224 - loss: 1.2934\n",
            "Epoch 4: val_accuracy improved from 0.53894 to 0.54577, saving model to /content/drive/MyDrive/AI_Checkpoints/emotion_model_optimized.weights.h5\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 17ms/step - accuracy: 0.5224 - loss: 1.2934 - val_accuracy: 0.5458 - val_loss: 1.2261 - learning_rate: 0.0010\n",
            "Epoch 5/15\n",
            "\u001b[1m2615/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5245 - loss: 1.2790\n",
            "Epoch 5: val_accuracy improved from 0.54577 to 0.55319, saving model to /content/drive/MyDrive/AI_Checkpoints/emotion_model_optimized.weights.h5\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 16ms/step - accuracy: 0.5245 - loss: 1.2790 - val_accuracy: 0.5532 - val_loss: 1.2128 - learning_rate: 0.0010\n",
            "Epoch 6/15\n",
            "\u001b[1m2613/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5357 - loss: 1.2511\n",
            "Epoch 6: val_accuracy improved from 0.55319 to 0.55407, saving model to /content/drive/MyDrive/AI_Checkpoints/emotion_model_optimized.weights.h5\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 16ms/step - accuracy: 0.5357 - loss: 1.2511 - val_accuracy: 0.5541 - val_loss: 1.2123 - learning_rate: 0.0010\n",
            "Epoch 7/15\n",
            "\u001b[1m2616/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5383 - loss: 1.2322\n",
            "Epoch 7: val_accuracy improved from 0.55407 to 0.55758, saving model to /content/drive/MyDrive/AI_Checkpoints/emotion_model_optimized.weights.h5\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 16ms/step - accuracy: 0.5383 - loss: 1.2322 - val_accuracy: 0.5576 - val_loss: 1.2077 - learning_rate: 0.0010\n",
            "Epoch 8/15\n",
            "\u001b[1m2614/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5501 - loss: 1.2177\n",
            "Epoch 8: val_accuracy improved from 0.55758 to 0.56363, saving model to /content/drive/MyDrive/AI_Checkpoints/emotion_model_optimized.weights.h5\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 15ms/step - accuracy: 0.5501 - loss: 1.2177 - val_accuracy: 0.5636 - val_loss: 1.2070 - learning_rate: 0.0010\n",
            "Epoch 9/15\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5520 - loss: 1.2100\n",
            "Epoch 9: val_accuracy did not improve from 0.56363\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 16ms/step - accuracy: 0.5520 - loss: 1.2100 - val_accuracy: 0.5621 - val_loss: 1.2076 - learning_rate: 0.0010\n",
            "Epoch 10/15\n",
            "\u001b[1m2616/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5616 - loss: 1.1906\n",
            "Epoch 10: val_accuracy did not improve from 0.56363\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 15ms/step - accuracy: 0.5616 - loss: 1.1906 - val_accuracy: 0.5589 - val_loss: 1.2076 - learning_rate: 0.0010\n",
            "Epoch 11/15\n",
            "\u001b[1m2615/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5606 - loss: 1.1807\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\n",
            "Epoch 11: val_accuracy did not improve from 0.56363\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 15ms/step - accuracy: 0.5606 - loss: 1.1807 - val_accuracy: 0.5636 - val_loss: 1.2098 - learning_rate: 0.0010\n",
            "Epoch 12/15\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5733 - loss: 1.1465\n",
            "Epoch 12: val_accuracy improved from 0.56363 to 0.57632, saving model to /content/drive/MyDrive/AI_Checkpoints/emotion_model_optimized.weights.h5\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 15ms/step - accuracy: 0.5733 - loss: 1.1465 - val_accuracy: 0.5763 - val_loss: 1.1813 - learning_rate: 5.0000e-04\n",
            "Epoch 13/15\n",
            "\u001b[1m2614/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5864 - loss: 1.1044\n",
            "Epoch 13: val_accuracy improved from 0.57632 to 0.58023, saving model to /content/drive/MyDrive/AI_Checkpoints/emotion_model_optimized.weights.h5\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 16ms/step - accuracy: 0.5864 - loss: 1.1044 - val_accuracy: 0.5802 - val_loss: 1.1770 - learning_rate: 5.0000e-04\n",
            "Epoch 14/15\n",
            "\u001b[1m2614/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5954 - loss: 1.0867\n",
            "Epoch 14: val_accuracy improved from 0.58023 to 0.58208, saving model to /content/drive/MyDrive/AI_Checkpoints/emotion_model_optimized.weights.h5\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 17ms/step - accuracy: 0.5954 - loss: 1.0867 - val_accuracy: 0.5821 - val_loss: 1.1867 - learning_rate: 5.0000e-04\n",
            "Epoch 15/15\n",
            "\u001b[1m2617/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6040 - loss: 1.0698\n",
            "Epoch 15: val_accuracy improved from 0.58208 to 0.58813, saving model to /content/drive/MyDrive/AI_Checkpoints/emotion_model_optimized.weights.h5\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 16ms/step - accuracy: 0.6040 - loss: 1.0698 - val_accuracy: 0.5881 - val_loss: 1.1763 - learning_rate: 5.0000e-04\n",
            "Restoring model weights from the end of the best epoch: 15.\n",
            "\n",
            "==================================================\n",
            "🔧 PHASE 2 : Fine-tuning\n",
            "==================================================\n",
            "Epoch 15/30\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5403 - loss: 1.2572\n",
            "Epoch 15: val_accuracy improved from 0.58813 to 0.59145, saving model to /content/drive/MyDrive/AI_Checkpoints/emotion_model_optimized.weights.h5\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 29ms/step - accuracy: 0.5403 - loss: 1.2572 - val_accuracy: 0.5915 - val_loss: 1.1447 - learning_rate: 5.0000e-05\n",
            "Epoch 16/30\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5887 - loss: 1.1177\n",
            "Epoch 16: val_accuracy improved from 0.59145 to 0.60619, saving model to /content/drive/MyDrive/AI_Checkpoints/emotion_model_optimized.weights.h5\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 17ms/step - accuracy: 0.5887 - loss: 1.1177 - val_accuracy: 0.6062 - val_loss: 1.1134 - learning_rate: 5.0000e-05\n",
            "Epoch 17/30\n",
            "\u001b[1m2616/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6066 - loss: 1.0650\n",
            "Epoch 17: val_accuracy improved from 0.60619 to 0.61331, saving model to /content/drive/MyDrive/AI_Checkpoints/emotion_model_optimized.weights.h5\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 19ms/step - accuracy: 0.6066 - loss: 1.0650 - val_accuracy: 0.6133 - val_loss: 1.0881 - learning_rate: 5.0000e-05\n",
            "Epoch 18/30\n",
            "\u001b[1m2616/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6237 - loss: 1.0124\n",
            "Epoch 18: val_accuracy improved from 0.61331 to 0.61878, saving model to /content/drive/MyDrive/AI_Checkpoints/emotion_model_optimized.weights.h5\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 17ms/step - accuracy: 0.6237 - loss: 1.0124 - val_accuracy: 0.6188 - val_loss: 1.0705 - learning_rate: 5.0000e-05\n",
            "Epoch 19/30\n",
            "\u001b[1m2617/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6368 - loss: 0.9852\n",
            "Epoch 19: val_accuracy improved from 0.61878 to 0.62620, saving model to /content/drive/MyDrive/AI_Checkpoints/emotion_model_optimized.weights.h5\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 17ms/step - accuracy: 0.6368 - loss: 0.9852 - val_accuracy: 0.6262 - val_loss: 1.0594 - learning_rate: 5.0000e-05\n",
            "Epoch 20/30\n",
            "\u001b[1m2615/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6475 - loss: 0.9545\n",
            "Epoch 20: val_accuracy improved from 0.62620 to 0.63010, saving model to /content/drive/MyDrive/AI_Checkpoints/emotion_model_optimized.weights.h5\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 18ms/step - accuracy: 0.6475 - loss: 0.9545 - val_accuracy: 0.6301 - val_loss: 1.0515 - learning_rate: 5.0000e-05\n",
            "Epoch 21/30\n",
            "\u001b[1m2615/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6540 - loss: 0.9439\n",
            "Epoch 21: val_accuracy improved from 0.63010 to 0.63117, saving model to /content/drive/MyDrive/AI_Checkpoints/emotion_model_optimized.weights.h5\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 17ms/step - accuracy: 0.6540 - loss: 0.9439 - val_accuracy: 0.6312 - val_loss: 1.0478 - learning_rate: 5.0000e-05\n",
            "Epoch 22/30\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6584 - loss: 0.9199\n",
            "Epoch 22: val_accuracy improved from 0.63117 to 0.63400, saving model to /content/drive/MyDrive/AI_Checkpoints/emotion_model_optimized.weights.h5\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 18ms/step - accuracy: 0.6584 - loss: 0.9199 - val_accuracy: 0.6340 - val_loss: 1.0439 - learning_rate: 5.0000e-05\n",
            "Epoch 23/30\n",
            "\u001b[1m2615/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6656 - loss: 0.9003\n",
            "Epoch 23: val_accuracy improved from 0.63400 to 0.63420, saving model to /content/drive/MyDrive/AI_Checkpoints/emotion_model_optimized.weights.h5\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 18ms/step - accuracy: 0.6656 - loss: 0.9003 - val_accuracy: 0.6342 - val_loss: 1.0584 - learning_rate: 5.0000e-05\n",
            "Epoch 24/30\n",
            "\u001b[1m2616/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6740 - loss: 0.8817\n",
            "Epoch 24: val_accuracy improved from 0.63420 to 0.63508, saving model to /content/drive/MyDrive/AI_Checkpoints/emotion_model_optimized.weights.h5\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 19ms/step - accuracy: 0.6740 - loss: 0.8817 - val_accuracy: 0.6351 - val_loss: 1.0537 - learning_rate: 5.0000e-05\n",
            "Epoch 25/30\n",
            "\u001b[1m2616/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6823 - loss: 0.8551\n",
            "Epoch 25: val_accuracy improved from 0.63508 to 0.63957, saving model to /content/drive/MyDrive/AI_Checkpoints/emotion_model_optimized.weights.h5\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 17ms/step - accuracy: 0.6823 - loss: 0.8551 - val_accuracy: 0.6396 - val_loss: 1.0400 - learning_rate: 5.0000e-05\n",
            "Epoch 26/30\n",
            "\u001b[1m2615/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6866 - loss: 0.8441\n",
            "Epoch 26: val_accuracy improved from 0.63957 to 0.64318, saving model to /content/drive/MyDrive/AI_Checkpoints/emotion_model_optimized.weights.h5\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 19ms/step - accuracy: 0.6866 - loss: 0.8441 - val_accuracy: 0.6432 - val_loss: 1.0503 - learning_rate: 5.0000e-05\n",
            "Epoch 27/30\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6929 - loss: 0.8265\n",
            "Epoch 27: val_accuracy did not improve from 0.64318\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 16ms/step - accuracy: 0.6929 - loss: 0.8265 - val_accuracy: 0.6419 - val_loss: 1.0481 - learning_rate: 5.0000e-05\n",
            "Epoch 28/30\n",
            "\u001b[1m2614/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7013 - loss: 0.8043\n",
            "Epoch 28: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
            "\n",
            "Epoch 28: val_accuracy improved from 0.64318 to 0.64689, saving model to /content/drive/MyDrive/AI_Checkpoints/emotion_model_optimized.weights.h5\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 18ms/step - accuracy: 0.7013 - loss: 0.8043 - val_accuracy: 0.6469 - val_loss: 1.0556 - learning_rate: 5.0000e-05\n",
            "Epoch 29/30\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7084 - loss: 0.7894\n",
            "Epoch 29: val_accuracy did not improve from 0.64689\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 17ms/step - accuracy: 0.7084 - loss: 0.7894 - val_accuracy: 0.6465 - val_loss: 1.0490 - learning_rate: 2.5000e-05\n",
            "Epoch 30/30\n",
            "\u001b[1m2615/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7119 - loss: 0.7715\n",
            "Epoch 30: val_accuracy improved from 0.64689 to 0.65216, saving model to /content/drive/MyDrive/AI_Checkpoints/emotion_model_optimized.weights.h5\n",
            "\u001b[1m2618/2618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 17ms/step - accuracy: 0.7119 - loss: 0.7715 - val_accuracy: 0.6522 - val_loss: 1.0524 - learning_rate: 2.5000e-05\n",
            "Restoring model weights from the end of the best epoch: 30.\n",
            "\n",
            "==================================================\n",
            "📊 ÉVALUATION FINALE\n",
            "==================================================\n",
            "✅ Meilleurs poids chargés depuis le checkpoint.\n",
            "\u001b[1m641/641\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - accuracy: 0.5952 - loss: 1.2156\n",
            "\n",
            "🎯 Précision Finale sur le set de test : 65.22%\n",
            "💾 Modèle complet sauvegardé sur Google Drive.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "2314"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# UPGRADED SCRIPT: AIMING FOR 70%+ ACCURACY ON KAGGLE (COMPLETE & CORRECTED)\n",
        "# ==============================================================================\n",
        "#\n",
        "# STRATEGY:\n",
        "# 1. Using a more powerful EfficientNetV2B2 base model.\n",
        "# 2. Using larger 192x192 images for more detail.\n",
        "# 3. Using the robust AdamW optimizer to ensure stability.\n",
        "# 4. Deeper fine-tuning for better adaptation to the dataset.\n",
        "#\n",
        "# ==============================================================================\n",
        "\n",
        "# --- 1. SETUP AND IMPORTS ---\n",
        "import os\n",
        "import gc\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, applications, regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "\n",
        "# --- 2. DATASET DOWNLOAD FROM GOOGLE DRIVE ---\n",
        "!pip install -q gdown\n",
        "gdrive_link = \"https://drive.google.com/file/d/1oaJcF-Oe-81OD9wp16VExOJgLuc8vU94/view?usp=drive_link\"\n",
        "output_zip_path = \"/kaggle/working/dataset.zip\"\n",
        "extract_path = \"/kaggle/working/datasets/\"\n",
        "print(\"📂 Téléchargement du dataset depuis Google Drive...\")\n",
        "!gdown --fuzzy \"{gdrive_link}\" -O \"{output_zip_path}\"\n",
        "print(\"📦 Décompression du dataset...\")\n",
        "!unzip -q -o \"{output_zip_path}\" -d \"{extract_path}\"\n",
        "print(f\"✅ Dataset prêt dans {extract_path}\")\n",
        "\n",
        "\n",
        "# --- 3. CONFIGURATION AND OPTIMIZATIONS ---\n",
        "\n",
        "# --- AMÉLIORATION : Images plus grandes pour de meilleurs résultats ---\n",
        "IMG_SIZE = 192\n",
        "BATCH_SIZE = 32 # BATCH_SIZE de 32 est bon pour les GPU de Kaggle\n",
        "CHANNELS = 3\n",
        "EPOCHS_PHASE_1 = 15\n",
        "EPOCHS_PHASE_2 = 30\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# --- Paths configured for Kaggle ---\n",
        "TRAIN_DIR = os.path.join(extract_path, \"train\")\n",
        "TEST_DIR = os.path.join(extract_path, \"test\")\n",
        "CHECKPOINT_DIR = \"/kaggle/working/AI_Checkpoints\"\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, \"emotion_efficientnetv2B2.weights.h5\")\n",
        "\n",
        "# --- Performance Optimization ---\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "print(\"\\n✅ Mixed Precision Training Enabled.\")\n",
        "print(f\"📐 Image Resolution: {IMG_SIZE}x{IMG_SIZE}\")\n",
        "print(f\"📂 Chemin d'entraînement : {TRAIN_DIR}\")\n",
        "print(f\"💾 Chemin de sauvegarde : {CHECKPOINT_DIR}\")\n",
        "\n",
        "\n",
        "# --- 4. DATA PREPARATION AND AUGMENTATION ---\n",
        "\n",
        "# --- AMÉLIORATION : Ajout de RandomTranslation pour plus de robustesse ---\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.1),\n",
        "    layers.RandomZoom(0.1),\n",
        "    layers.RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
        "    layers.RandomContrast(0.1),\n",
        "], name=\"data_augmentation\")\n",
        "\n",
        "def create_dataset(directory, augment=False):\n",
        "    \"\"\"Loads, preprocesses, and augments data, returning the dataset and class names.\"\"\"\n",
        "    initial_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "        directory,\n",
        "        label_mode='categorical',\n",
        "        image_size=(IMG_SIZE, IMG_SIZE),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True if augment else False\n",
        "    )\n",
        "    class_names = initial_ds.class_names\n",
        "    dataset = initial_ds\n",
        "    if augment:\n",
        "        dataset = dataset.map(\n",
        "            lambda x, y: (data_augmentation(x, training=True), y),\n",
        "            num_parallel_calls=AUTOTUNE\n",
        "        )\n",
        "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "    return dataset, class_names\n",
        "\n",
        "print(\"\\nLoading and preparing datasets...\")\n",
        "train_dataset, class_names = create_dataset(TRAIN_DIR, augment=True)\n",
        "test_dataset, _ = create_dataset(TEST_DIR, augment=False)\n",
        "NUM_CLASSES = len(class_names)\n",
        "print(f\"✅ Found {NUM_CLASSES} classes: {class_names}\")\n",
        "\n",
        "\n",
        "# --- 5. MODEL DEFINITION (TRANSFER LEARNING) ---\n",
        "\n",
        "def build_model(input_shape, num_classes):\n",
        "    \"\"\"Builds the model using a more powerful EfficientNetV2B2 as a base.\"\"\"\n",
        "    # --- AMÉLIORATION : Utilisation du modèle B2, plus performant ---\n",
        "    base_model = applications.EfficientNetV2B2(\n",
        "        input_shape=input_shape,\n",
        "        include_top=False,\n",
        "        weights=\"imagenet\"\n",
        "    )\n",
        "    base_model.trainable = False\n",
        "    \n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = base_model(inputs, training=False)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    \n",
        "    # Une tête simple mais efficace\n",
        "    x = layers.Dense(512, activation=\"gelu\", kernel_regularizer=regularizers.l2(1e-4))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    \n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\", dtype=tf.float32)(x)\n",
        "    model = models.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "print(\"\\nBuilding model with EfficientNetV2B2 base...\")\n",
        "INPUT_SHAPE = (IMG_SIZE, IMG_SIZE, CHANNELS)\n",
        "model = build_model(INPUT_SHAPE, NUM_CLASSES)\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# --- 6. TRAINING STRATEGY (TWO-PHASE FINE-TUNING) ---\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_accuracy', patience=7, restore_best_weights=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, min_lr=1e-7, verbose=1),\n",
        "    ModelCheckpoint(filepath=CHECKPOINT_PATH, monitor='val_accuracy', save_best_only=True,\n",
        "                    save_weights_only=True, mode='max', verbose=1)\n",
        "]\n",
        "\n",
        "# --- PHASE 1: Feature Extraction ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"🚀 PHASE 1: Training the Classification Head\")\n",
        "print(\"=\"*50)\n",
        "# --- AMÉLIORATION : Optimiseur AdamW avec un learning rate sûr pour la stabilité ---\n",
        "model.compile(\n",
        "    optimizer=AdamW(learning_rate=5e-4, weight_decay=1e-4, clipnorm=1.0),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "history_phase1 = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=test_dataset,\n",
        "    epochs=EPOCHS_PHASE_1,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# --- PHASE 2: Fine-Tuning ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"🔧 PHASE 2: Deeper Fine-Tuning\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "base_model = model.layers[1]\n",
        "base_model.trainable = True\n",
        "\n",
        "# --- AMÉLIORATION : Dégeler les derniers 40% des couches pour un fine-tuning plus profond ---\n",
        "fine_tune_at = int(len(base_model.layers) * 0.60)\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# --- CORRECTION DE LA LIGNE INCOMPLÈTE ---\n",
        "print(f\"🔓 Unfrozen {len(base_model.layers) - fine_tune_at} layers out of {len(base_model.layers)} total.\")\n",
        "\n",
        "# On recompile le modèle avec un taux d'apprentissage très bas pour le fine-tuning\n",
        "model.compile(\n",
        "    optimizer=AdamW(learning_rate=2e-5, weight_decay=1e-5, clipnorm=1.0),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "history_phase2 = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=test_dataset,\n",
        "    epochs=EPOCHS_PHASE_2,\n",
        "    initial_epoch=history_phase1.epoch[-1] if history_phase1.epoch else 0,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# --- 7. FINAL EVALUATION AND SAVING ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"📊 FINAL EVALUATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "if os.path.exists(CHECKPOINT_PATH):\n",
        "    model.load_weights(CHECKPOINT_PATH)\n",
        "    print(\"✅ Best weights loaded from checkpoint for final evaluation.\")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_dataset, verbose=1)\n",
        "print(f\"\\n🎯 Final Test Accuracy: {test_acc*100:.2f}%\")\n",
        "print(f\"📉 Final Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "final_model_name = f\"emotion_model_final_acc_{test_acc*100:.2f}.keras\"\n",
        "final_model_path = os.path.join(CHECKPOINT_DIR, final_model_name)\n",
        "model.save(final_model_path)\n",
        "print(f\"💾 Model saved to: {final_model_path}\")\n",
        "\n",
        "gc.collect()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
